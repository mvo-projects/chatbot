# initialize the mode ("train" or "eval")
# set use_cuda to yes if you want to use the GPU, no otherwise
[init]
mode = eval
use_cuda = no

# Filtering the data to only keep sentences between [$min_length;$max_length]
# A sentence is considered to be in the same context than the precedent one only if there is less than delay seconds separating them
# Each vocabulary word has to appear at least "$trim_min_count" times to be a valid one. (that softens spelling errors in the data)
# set use_qacorpus to yes if you are using a pre-established Question/Answer data
# set create_qapairs to yes if you want to take only account of questions as valid contexts
# set corpuspaths to the differents locations of the corpus separated by a comma ',' if multiples
# set qacorpuspaths to create pairs from the differents locations of the corpus
# set qapairspath if you are using pre-established QA data
[data]
min_length = 2
max_length = 13
delay = 5
trim_min_count = 10
use_qacorpus = yes
create_qapairs = no
qacorpuspaths = ./OpenSubtitles2018/xml/fr
qapairspath = qa_totalpairs.txt
corpuspaths = ./OpenSubtitles2018/xml/fr/2016, ./OpenSubtitles2018/xml/fr/2017

# parameters of the model
# Luong attention model - methods "dot", "general" or "concat" (https://arxiv.org/pdf/1508.04025.pdf)
# right now you can only initialize the optimizer to SGD or Adam from this config file, 
# set criterion to either "CrossEntropyLoss" or "NLLLoss"
[model]
attn_model = dot
hidden_size = 1024
n_layers = 2
dropout = 0.3
learning_rate = 0.0001
decoder_learning_ratio = 5.0
optimizer = Adam
criterion = CrossEntropyLoss

# set load_training to yes if you want to load a pretrained model, make sure you load the configure the parameters of the model too.
# if load_training is set to yes, add the encoder and decoder paths (absolute or relative paths)
[load]
load_training = yes
load_encoderpath = ./encoderQA
load_decoderpath = ./decoderQA

# begin training session at '$iteration'
# stop training session at '$n_iterations'
# save model every '$save_every' iterations
# print details of the training every '$print_every' iterations
# evaluate the model with context from data every '$evaluate_every' iterations and compare with real answer 
# '$save_encoderpath' and '$save_decoderpath' is the name of the model in the end
[training]
batch_size = 128
clip = 50.0
teacher_forcing_ratio = 0.5
iteration = 0
n_iterations = 100000
save_every = 10000
print_every = 100
evaluate_every = 1000
save_encoderpath = encoder_QA_nocaps_accents
save_decoderpath = decoder_QA_nocaps_accents

# max_length is the maximal length of the output sentence
# temperature_module_name is the name of the module where we put temperatures functions
# temperature_function_name is the name of the temperature function you want to use inside the module $temperature_module_name
# during the evaluation and the creation of the output sentence, for each word created we've considered "$n_best_words" possibilities. The more you train your model, the more this parameter can go higher and give you more flexibility.
[eval]
max_length = 13
temperature_module_name = temp
temperature_function_name = inverse_fun
n_best_words = 10
